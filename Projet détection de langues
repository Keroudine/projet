# Projet dÃ©tection de langues.
# Les participants:
    Keroudine BELLADJO & Ameline FAVRAIS
En premier lieu, afin que les fonctions puissent fonctionner, il faut importer diffÃ©rentes bibliothÃ¨ques de pyhton, Ã  savoir:
- Matplotlib qui va nous permettre de visualiser les donnÃ©es (tracer des graphiques)
- Scikit-learn qui propose une implÃ©mentation de lâ€™algorithme des k plus proches voisins.
- Pandas qui va nous permettre de traiter nos donnÃ©es
- numpy permet d'effectuer des calculs numÃ©riques et introduit une gestion facilitÃ©e des tableaux de nombres
- requests Gestion domaines et URLS internationales
- re  permet d'utiliser des expressions rÃ©guliÃ¨res avec Python
- BeautifulSoup (belle soup)- BeautifulSoup (belle soup) est une bibliothÃ¨que Python d'analyse syntaxique de documents HTML et XML
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import math
import requests
import re
from bs4 import BeautifulSoup
from sklearn.model_selection import train_test_split
from sklearn.metrics import zero_one_loss
from sklearn.neighbors import KNeighborsClassifier
# PARTIE 1
## 1) CrÃ©ation d'un ensemble de donnÃ©es en 5 langues.
On importe des morceaux de texte suffisemment grand pour etre Ã©tudiÃ©, et de diffÃ©rentes langues afin de pouvoir comparer les caractÃ¨res par la suite.
- Voici les url du meme texte en 5 langues et leur classe :

url=("https://fr.wikipedia.org/wiki/Martin_Luther_King",
"https://en.wikipedia.org/wiki/Martin_Luther_King_Jr.",
"https://de.wikipedia.org/wiki/Martin_Luther_King",
"https://it.wikipedia.org/wiki/Martin_Luther_King",
"https://es.wikipedia.org/wiki/Martin_Luther_King")
classe=("FR","EN","DE","IT","ES")
- Nous allons dÃ©finir une fonction "Extraction_de_texte(url)" qui prend en argument un url et mets la contenance de cet url dans un fichier texte et le retourne. Cette fonction nous permetra par la suite de pouvoir traiter la donnÃ©e que contient l'url.
def extraction_texte(url):
    """
     Fonction qui prend en argument un url,le traite
     et retourne son contenue
    """
    page=requests.get(url)
    p=BeautifulSoup(page.content,"html.parser")
    resultat=p.find(id="mw-content-text")
    para=resultat.find_all("p")
    l=[]
    for t in para:
        l.append(t.text)
    return " ".join(l)
- On a creer la fonction "texte_sans_c_speciaux" pour filtrer tous les caractÃ¨res spÃ©ciaux et accentuÃ©s qui sont integrÃ©s dans les textes, comme par exemple les parenthÃ¨ses, en effet cela n'est pas utile de faire une comparaison sur ce genre de caractÃ¨res et cela la rendra plus prÃ©cise. On limitera Ã©galement les textes Ã  40 000 caractÃ¨res afin que tout les textes aient le meme nombre de fragments n.
def texte_sans_c_speciaux(t,ind_d_ajstmnt_text=40000):
     
    """Fonction qui prend en argument un fichier texte creer par la fonction precedente et un
    Indice d'ajustement de la longueur du texte et 
    retourne toutes les sous-chaÃ®nes qui correspondent Ã  des lettres muniscules ou majuscules du fichier texte"""
    
    texte=t.lower()
    texte=re.findall("[a-z]",texte)
    texte="".join(texte)
    if len(texte)> ind_d_ajstmnt_text:
        texte= texte[0:ind_d_ajstmnt_text]
    else:
        print("Indice d'ajustement de la longueur du texte est trop grand " + str(len(texte)))

    return texte
- CrÃ©action final des textes avec les 2 fonctions prÃ©cedentes pour les 5 langues.
texte_fr=texte_sans_c_speciaux(extraction_texte(url[0]))
texte_en=texte_sans_c_speciaux(extraction_texte(url[1]))
texte_de=texte_sans_c_speciaux(extraction_texte(url[2]))
texte_it=texte_sans_c_speciaux(extraction_texte(url[3]))
texte_es=texte_sans_c_speciaux(extraction_texte(url[4]))
## 2) Exploration des donnÃ©es.
- HypothÃ¨se: La distribution des caractÃ¨res varie d'une langue Ã  l'autre.
- Nous allons dans un premier temps Ã©tudier la longueur de chaque texte afin de vÃ©rifier qu'elle soit bien limitÃ© Ã  40 000 caractÃ¨res.
len(texte_fr),len(texte_en),len(texte_de),len(texte_es),len(texte_it)
- On peut donc creer un tableur, plus prÃ©cisement un DataFrame de chacune des 5 langues, qui donnera le nombre d'occurences de chacun des caractÃ¨res.
V=pd.Series(list(texte_it)).value_counts()
W=pd.Series(list(texte_es)).value_counts()
X=pd.Series(list(texte_fr)).value_counts()
Y=pd.Series(list(texte_en)).value_counts()
Z=pd.Series(list(texte_de)).value_counts()
df= pd.DataFrame(
{"Italien": V,
 "Espagnol": W,
 "FranÃ§ais": X,
"Anglais": Y,
"Allemand": Z})
- On remplacera Ã©galement les "NaN" par des 0.
df = df.replace(np.NaN, 0, regex=True)
df
- On peut ensuite ranger les caractÃ¨res par nombre d'occurences, par langue, de facon croissante, afin de pouvoir observer lesquelles sont plus frÃ©quentes et lesquelles ne le sont pas. 
df["FranÃ§ais"].sort_values()
df["Anglais"].sort_values()
df["Allemand"].sort_values()
df["Espagnol"].sort_values()
df["Italien"].sort_values()
# Partie 2 :
## ReprÃ©sentation des donnÃ©es.
- Contruction d'un diagrame en baton pour avoir la dispersion des caractÃ¨res pour chaque langue afin de determiner ceux qui sont les plus frÃ©quents et les plus dicriminants d'une langue Ã  l'autre.
- Il nous permettra par la suite de determiner les caractÃ¨res que nous allons utilisÃ©s pour notre ensemble d'entrainement que nous allons nommÃ© "d" par la suite
df.plot(kind='bar',figsize=[15,8])
- L'hypothÃ¨se est vÃ©rifiÃ©e, on peut observer Ã  travers ce graphique que la distribution des caractÃ¨res varie d'une langue Ã  l'autre.
- On pourra donc s'en servir pour faire de la dÃ©tection de langues.
- On va donc faire de l'exploration de donnÃ©es pour tester cette hypothÃ¨se:
    - quels sont les diffÃ©rents caractÃ¨res utilisÃ©s dans les diffÃ©rentes langues?
    - quelle est la frÃ©quence d'apparition de chacun des caractÃ¨res?

- On choisi donc un d en fonction des caractÃ¨res les plus frÃ©quents et discriminants. Ici on prendra les 4 caractÃ¨res les plus prÃ©sents dans chacune des langues. Ce qui rÃ©pondra Ã  notre premiÃ¨re question.
d=['a', 'e', 'n', 'u', 'p', 'i', 's', 'w','o', 'l', 'h','y','b','r','j','t'] 
## Constitution d'un ensemble d'entraÃ®nement.
## 1) Fragmentation des textes.
- Afin d'avoir un rÃ©sultat cohÃ©rent, on a prit comme l_min la racine carrÃ© de la longueurs des textes, ce qui donne donc l_min = 200. Cette maniÃ¨re de faire permet d'avoir une mÃªme taille de fragment pour tou le texte, ainsi cela permetra d'avoir une meilleurs precision pour la prediction de la langue.
l_min=math.ceil(np.sqrt(40000))
l_min
- La fonction que nous allons definir maintenant appelÃ©e "les_fragment_du_texte(url,l_min)"  va nous permetre de dÃ©couper les textes rÃ©coltÃ©s dans l'Ã©tape prÃ©cÃ©dente en fragments de textes qui seront les entrÃ©es d'un classifieur (ici l'algo de Knn)
    - On dipose du meme texte en 5 langues: FranÃ§ais,Anglais,Allemand,Italien,Espagnol. 
    - Pour chaque texte, on le dÃ©coupe par defaut en l_min=200 par morceaux pour avoir un total de n=N//l_min=200 fragments avec N la longueur du texte. 
    - Pour chaque entrÃ©e, on calcule la frequence de chaque caractÃ¨re.
    - N est suffisamment grand (N=40000) pour chaque texte pour ne pas avoir des fragments beaucoup trop petits
    - On a chosi par defaut l_min ou la longueur de fragemnt egale 200 parce qu'il semble plus interesant (peut minimiser le plus possible le risque empirique et en generalisation), c'est en effet notre l_min choisi, nous veront par la suite si notre intuition est juste.
def les_fragment_du_texte(url,l_min=200):
    """Prend en argument un url et l_min la longeur des fragment et
    retourne le texte fragmentÃ©  sous forme d'une liste de n fragment de longueur l_min 
    """
    texte=texte_sans_c_speciaux(extraction_texte(url))
    N=len(texte)
    n=N//l_min
    l=[]
    while n>0:
        n=n-1
        l.append(texte[:l_min])
        texte=texte[l_min:]
    return l
- Obtention des textes fragmenter des 5 langues.
frag_fr=les_fragment_du_texte(url[0],l_min)
frag_en=les_fragment_du_texte(url[1],l_min)
frag_de=les_fragment_du_texte(url[2],l_min)
frag_es=les_fragment_du_texte(url[3],l_min)
frag_it=les_fragment_du_texte(url[4],l_min)
len(frag_fr),len(frag_en),len(frag_de),len(frag_es),len(frag_it)
- RÃ©sultat cohÃ©rent; en effet les textes sont tous limitÃ©s Ã  40000 caractÃ¨res et les fragment Ã  40000/200 = 200
n=len(frag_fr)
n
- Conlusion:
    - d = ['a', 'e', 'n', 'u', 'p', 'i', 's', 'w','o', 'l', 'h','y','b','r','j','t'] Lettres les plus frÃ©quentes et discriminantes.
    - l_min = 200. Racine carrÃ© de 40 000.
    - N = 40 000. Texte suffisamment grand partagÃ© Ã©quitablement en 200 fragments de 200 caractÃ¨res.
    - n = âŒŠN/l_minâŒ‹ = 40 000/200 = 200
## 2) DÃ©finition des instance d'apprentissage.
Nous allons definir fonction "frequence(texte,list_of_car)" qui nous permettre de calculer la frequenche de chaque element de notre ensemble "d" dans les fragments du texte. 
Pour chaque entrÃ©e, on calcule la frÃ©quence de chaque caracte de notre ensemble "d". On a choisie donc d=['a', 'e', 'n', 'u', 'p', 'i', 's', 'w','o', 'l', 'h','y','b','r','j','t'] par defaut car c'est lui qui minismise nos risques le mieux parmis tous ceux q'on a essayer, nous le demontrerons egalement par la suite
def frequence(texte,list_of_car=d):
    """CrÃ©ation d'une fonction frÃ©quence qui prend en argument un texte et une liste de caractere,
    calcul la frequence de chaque caractere de la liste
    et retourne les frequences calculÃ©es sous forme de liste"""
    li=[]
    for c in list_of_car:
        li.append(texte.count(c)/len(texte))
    return li
- Exemple de la frÃ©quence de quelques lettres du texte en francais : 
frequence(texte_fr,["a","s","e","z"])
- CrÃ©ation de 5 dataframes contenant les frÃ©quences de chaque caractÃ¨re sur les 200 fragments pour chacune des 5 langues.
la fonction datafrag est une Fonction qui permet de constituer une liste de liste des frequences de caractere de chaque fragment d'un texte. elle nous permtra de pouvoir crÃ©er des dataframes pour chaque langue.
def datafrag(url,list_of_car,l_min):
    data=[]
    for frag in les_fragment_du_texte(url,l_min):
        data.append(frequence(frag,list_of_car))
    return data
La fonction que nous allons definir maintenant intitulÃ©e "Instances(url,list_of_car=d,l_min,classe)" nous permetra de construire un dataframe. Ce dataframe va etre composÃ© en ligne par la frequence des elements de de l'ensemnbe "d" des diferements frangemnt d'un texte et  en colone par les elemnts de l'ensemble "d" et la classe du fragment.
def instances(url,list_of_car,l_min,classe):
    frag=datafrag(url,list_of_car,l_min)
    data=pd.DataFrame(frag,columns=list_of_car)
    data["Classe"]=classe
    return data
- Dataframme contenant les frÃ©quences de chaque caractÃ¨re des 200 fragments en Francais :
data_fr=instances(url[0],d,l_min,classe[0])
data_fr
- Dataframme contenant les frÃ©quences de chaque caractÃ¨re des 202 fragments en Anglais :
data_en=instances(url[1],d,l_min,classe[1])
data_en
- Dataframme contenant les frÃ©quences de chaque caractÃ¨re des 202 fragments en Allemand :
data_de=instances(url[2],d,l_min,classe[2])
data_de
- Dataframme contenant les frÃ©quences de chaque caractÃ¨re des 202 fragments en Italien :
data_it=instances(url[3],d,l_min,classe[3])
data_it
- Dataframme contenant les frÃ©quences de chaque caractÃ¨re des 202 fragments en Espagnol :
data_es=instances(url[4],d,l_min,classe[4])
data_es
- CrÃ©ation d'un dataframme comprenant les 5 dataframmes vu prÃ©cedemment avec leur classe (concatÃ©nation).
La fonction data_instances(l) ci-dessous va nous permetre de concatener les dataframe des differentes langues construit a partir de la fonction "Instances" precedent. Elle permet donc de constituer notre ensemble d'entraÃ®nement (instances d'apprentissage) pour l'algo de knn.
def data_instances(l):
    data=pd.concat(l, axis=0)
    data=data.sort_values(by=['Classe'],ascending=False)
    return data
data=data_instances([data_fr,data_en,data_de,data_it,data_es])
data
- Ce dataframme a permit de rÃ©pondre Ã  notre seconde question et de donner la frÃ©quence de chaque caractÃ¨re ainsi que leur classe.
## 3) Erreur empirique et en gÃ©nÃ©ralisation.
- Le module dâ€™apprentissage scikit-learn propose une implÃ©mentation de lâ€™algorithme des k plus proches voisins.
- Ã‰valuation :

    - scikit-learn dispose de nombreuses mÃ©triques dâ€™Ã©valuation.

    - ComenÃ§ons donc par partager notre Ã©chantillon :
ur="https://fr.wikipedia.org/wiki/Niger"
ur1=url[0]
datafr=instances(ur1,d,l_min,classe[0])
p=datafr[:1]
p=p[d]
p
X_train, X_test, y_train, y_test = train_test_split(data[d],data["Classe"],test_size=0.4,random_state=9)
- Les paramÃ¨tres de la fonction train_test_split sont :

    - les donnÃ©es : variables prÃ©dictives ici data[d] (les fragments de texte)
    - les donnÃ©es : variable Ã  prÃ©dire ici data["Classe"] (les langues)
    - test_size : proportion de lâ€™Ã©chantillon consacrÃ© au test
    - random_state : graine du gÃ©nÃ©rateur alÃ©atoire utilisÃ© pour le dÃ©coupage
len(X_train),len(y_train)
X=data[d]
Y=data["Classe"]
X
Y
y=X[900:901] 
y
 knn = KNeighborsClassifier(n_neighbors=18)
knn.fit(X, Y)
knn.predict(y)
Nous avons maintenant definir la fonction Erreur qui nous permetra de calculer l'erreur empirique et en generalisation pour un k donnÃ©. Plus k est faible plus on aura une meilleur precision pour la prediction d'ou l'importance de cette fonction 
def erreur(list_of_car,l_min,k):
    l=[instances(url[0],d,l_min,classe[0]),
       instances(url[1],d,l_min,classe[1]),
       instances(url[2],d,l_min,classe[2]),
       instances(url[3],d,l_min,classe[3]),
       instances(url[4],d,l_min,classe[4])]
    data=data_instances(l)[list_of_car ]
    cible=data_instances(l)["Classe"]
    X_train, X_test, y_train, y_test = train_test_split(data,cible,test_size=0.4,random_state=11)
    
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    
    #Calcul de l'erreur empirique
    yhat_train = knn.predict(X_train) # predition de la langue  
    err_emp = zero_one_loss(yhat_train, y_train) # calcul combien de fois on se trompe dans notre prÃ©diction

    #Calcul  l'erreur en generalisation
    yhat_val = knn.predict(X_test)
    err_gen = zero_one_loss(yhat_val, y_test)

    print("Erreur empirique:", err_emp, 
          "\nErreur en gÃ©nÃ©ralisation:", err_gen)
La fonction Entrainement a le mÃªme fonctionnement que la fonction precedent sa seule particularitÃ©, il propose plusieurs k. Il nous permet d'avoir une liste des k pour en fin choisir celui qui minimise les risques d'erreur (empirique et en generalisation).
def Entrainement(l,n,list_of_car):
    
    data=data_instances(l)[list_of_car ]
    cible=data_instances(l)["Classe"]
    X_train, X_test, y_train, y_test = train_test_split(data,cible,test_size=0.4,random_state=11)
    erreurs = []
    for k in range(1,n):
        # L'entraÃ®nement du modÃ¨le
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, y_train) #entrainemnet du monele
        # l'erreur empirique
        yhat_train = knn.predict(X_train) # prediction de la langue 
        err_emp = zero_one_loss(yhat_train, y_train) # calcul combien de fois on se trompe dans notre prÃ©diction
        # l'erreur en generalisation
        yhat_val = knn.predict(X_test)
        err_gen = zero_one_loss(yhat_val, y_test)
        erreurs.append([k, err_emp, err_gen])
        dfErreurs = pd.DataFrame(erreurs, 
                                 columns=["k", "empirique", "generalization"])
    return dfErreurs
La Fonction f est un complement de la fonction "Entrainement". Elle fait la meme chose que cette derniere, elle nous permet tout simplement d'eviter de duppliquer cette fonction a chaque fois qu'on change de parametre.
def f(d,l_min,n):
    l=[instances(url[0],d,l_min,classe[0]),
       instances(url[1],d,l_min,classe[1]),
       instances(url[2],d,l_min,classe[2]),
       instances(url[3],d,l_min,classe[3]),
       instances(url[4],d,l_min,classe[4])]
    dataEr=Entrainement(l,n,d)
    return dataEr
f(d,l_min,20)
On trace les courbes des erreurs empirique et en generalisation pour visualiser leurs allure et choisir le plus petit k 
dfErreurs=f(d,l_min,20)
dfErreurs.plot(x="k",grid=True)
- Un classifier ğ‘˜-plus proches voisins avec ğ‘˜=19 semble donner le meilleur rÃ©sultat en terme de gÃ©nÃ©ralisation.
erreur(d,l_min,19)
- On chosi donc le k qui minimise l'erreur.
- En prenant le k=19 on observe un erreur moins importante pour l_min = 200
# Appronfondissement.
# 1 er cas Ã©tudiÃ©: prenons les memes instances mais avec des d et des l_min diffÃ©rents :
Maintenant gardons les memes instances, le meme d, mais prenons l_min=150
dfErreurs=f(d,150,20)
dfErreurs.plot(x="k",grid=True)
- Un classifier ğ‘˜-plus proches voisins avec ğ‘˜=12 semble donner le meilleur rÃ©sultat en terme de gÃ©nÃ©ralisation.
- On observe qu'ici les k sont diffÃ©rents lorsque l_min change.
erreur(d,150,12)
erreur(d,l_min,19)
 - le minimum des k pour l_min=150 est plus elevÃ© que quand l_min=200
Gardons les memes instances, le meme d, mais prenons l_min=250
dfErreurs=f(d,250,15)
dfErreurs.plot(x="k",grid=True)
- Un classifier ğ‘˜-plus proches voisins avec ğ‘˜=3 semble donner le meilleur rÃ©sultat en terme de gÃ©nÃ©ralisation
erreur(d,250,3)
erreur(d,l_min,19)
- Pour l_min=250 on se trompe moins par raport l_min=200
- On observe donc que selon le l_min l'erreur est plus ou moins importante.
Maintenant changeons simplement d et gardons notre l_min de base.
d2=['p','i','t','s']
dfErreurs=f(d2,l_min,20)
dfErreurs.plot(x="k",grid=True)
- Un classifier ğ‘˜-plus proches voisins avec ğ‘˜=18 semble donner le meilleur rÃ©sultat en terme de gÃ©nÃ©ralisation.
erreur(d2,l_min,18)
erreur(d,l_min,19)
- On observe une erreur plus importante pour ce d choisi. 
- Conclusion 1 :
    - Avec les 5 instances on observe que l'erreur devient plus ou moins importante lorsque d ou l_min change.
# 2 Ã¨me cas: On compare les erreurs entre 2 langues proches et 2 langues diffÃ©rentes :
Maintenant changeons les instances (uniquement espagnol et italien, langues qui se rapprochent) en gardant le meme l_min et le meme d :
def erreur2(list_of_car,l_min,k):
    l=[instances(url[3],d,l_min,classe[3]),
       instances(url[4],d,l_min,classe[4])]
    data=data_instances(l)[list_of_car ]
    cible=data_instances(l)["Classe"]
    X_train, X_test, y_train, y_test = train_test_split(data,cible,test_size=0.4,random_state=11)
    
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    
    #Calcul de l'erreur empirique
    yhat_train = knn.predict(X_train) # predition de la langue  
    err_emp = zero_one_loss(yhat_train, y_train) # calcul combien de fois on se trompe dans notre prÃ©diction

    #Calcul  l'erreur en generalisation
    yhat_val = knn.predict(X_test)
    err_gen = zero_one_loss(yhat_val, y_test)

    print("Erreur empirique:", err_emp, 
          "\nErreur en gÃ©nÃ©ralisation:", err_gen)
def f_es_it(d,l_min,n):
    l=[instances(url[3],d,l_min,classe[3]),
       instances(url[4],d,l_min,classe[4])]
    dataEr=Entrainement(l,n,d)
    return dataEr
dfErreurs=f_es_it(d,l_min,20)
dfErreurs.plot(x="k",grid=True)
- Un classifier ğ‘˜-plus proches voisins avec ğ‘˜=9 semble donner le meilleur rÃ©sultat en terme de gÃ©nÃ©ralisation.
erreur2(d,l_min,9)
erreur(d,l_min,19)
- On constate que pour le cas ou il y a tout les instances l'erreur empirique est moins importante.
Maintenant prenons uniquement francais et allemand ( langue diffÃ©rente ) et comparons avec notre exemple prÃ©cÃ©dent.
def erreur3(list_of_car,l_min,k):
    l=[instances(url[0],d,l_min,classe[0]),
       instances(url[2],d,l_min,classe[2])]
    data=data_instances(l)[list_of_car ]
    cible=data_instances(l)["Classe"]
    X_train, X_test, y_train, y_test = train_test_split(data,cible,test_size=0.4,random_state=11)
    
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    
    #Calcul de l'erreur empirique
    yhat_train = knn.predict(X_train) # predition de la langue  
    err_emp = zero_one_loss(yhat_train, y_train) # calcul combien de fois on se trompe dans notre prÃ©diction

    #Calcul  l'erreur en generalisation
    yhat_val = knn.predict(X_test)
    err_gen = zero_one_loss(yhat_val, y_test)

    print("Erreur empirique:", err_emp, 
          "\nErreur en gÃ©nÃ©ralisation:", err_gen)
def f_fr_de(d,l_min,n):
    l=[instances(url[0],d,l_min,classe[0]),
       instances(url[2],d,l_min,classe[2])]
    dataEr=Entrainement(l,n,d)
    return dataEr
dfErreurs=f_fr_de(d,l_min,20)
dfErreurs.plot(x="k",grid=True)
- Un classifier ğ‘˜-plus proches voisins avec ğ‘˜=11 semble donner le meilleur rÃ©sultat en terme de gÃ©nÃ©ralisation.
- Conclusion :
    - Lorsque les langues se rapprochent il y a plus d'erreur, en effet cela s'explique par le fait qu'il est difficile de les reconnaitres en sachant qu'elles ont des similitudes.
## Conclusion:
- Il faut donc choisir le l_min, le d, et les instances qui minise les risques; en effet certains cas comme on a pu Ã©tudiÃ©, comme par exemple en modifiant le domaine instances on remarque des risques d'erreurs plus ou moins  importants.
-les langues les plus proches semble donnÃ©e des risques d'erreurs k plus elevÃ©s que celles qui ne le sont pas, la prediction pour ces langues est donc moins precises.

- Parmis tous les cas qu'on a pu etudier, le cas oÃ¹ l_min=250 avec d=['a', 'e', 'n', 'u', 'p', 'i', 's', 'w','o', 'l', 'h','y','b','r','j','t']  et n=160 semble donne un meilleurs resultat (le risque d'erreur est beaucoup plus faible)
